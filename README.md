# Understanding Deep Learning - Exercises & Notes

A comprehensive learning journey through deep learning fundamentals based on the **"Understanding Deep Learning"** textbook by Simon Prince.

ğŸ“– **Official Resource:** [https://udlbook.github.io/udlbook/](https://udlbook.github.io/udlbook/)

---

## ğŸ“š Completed Exercises

### **Chapter 2: Supervised Learning**

#### 2.1 Linear Regression âœ…
- **File:** `2_1_Supervised_Learning.ipynb`
- **Topics Covered:**
  - 1D linear regression model: f(x) = Ï†â‚€ + Ï†â‚Â·x
  - Loss computation (sum-of-squares)
  - Partial derivatives: âˆ‚L/âˆ‚Ï†â‚€ and âˆ‚L/âˆ‚Ï†â‚
  - Gradient descent optimization
  - Parameter history tracking during training
  - **Loss landscape visualization:**
    - Heatmap showing loss at each (Ï†â‚€, Ï†â‚) combination
    - Contour lines for constant-loss regions
    - Gradient descent trajectory (red path)
    - Start point (green circle) and optimal point (red star)
- **Key Learnings:**
  - How to construct a design matrix with bias term
  - Matrix multiplication for batch predictions
  - Computing gradients analytically
  - Effect of learning rate on convergence
  - Visual interpretation of optimization in parameter space

#### Differentiation & Loss Functions (Supporting Work) âœ…
- **File:** `differentiation_loss_wrt_parameter.ipynb`
- **Topics Covered:**
  - Design matrix construction: X = [1 | x]
  - Mean Squared Error (MSE) loss function
  - Analytical gradient computation
  - Numerical gradient verification (finite differences)
  - Gradient descent from scratch
  - Loss landscape contour visualization
- **Key Learnings:**
  - Understanding the @ operator (matrix multiplication)
  - Verification: analytical gradients â‰ˆ numerical gradients
  - Convex nature of MSE loss
  - Parameter convergence behavior

---

## ğŸ“Š Progress Tracking

| Chapter | Section | Topic | Status | Completion Date |
|---------|---------|-------|--------|-----------------|
| 2 | 2.1 | Linear Regression | âœ… Complete | Nov 30, 2025 |
| 2 | 2.2 | Multivariate Linear Regression | â³ Pending | â€” |
| 2 | 2.3 | Loss Functions | â³ Pending | â€” |
| 3 | 3.1 | Shallow Neural Networks | â³ Pending | â€” |
| 3 | 3.2 | Activation Functions | â³ Pending | â€” |
| 4 | 4.1 | Backpropagation | â³ Pending | â€” |

---

## ğŸ”§ Setup & Requirements

```bash
# Install dependencies
pip install numpy matplotlib scipy

# Run notebooks
jupyter notebook
```

---

## ğŸ’¡ Key Concepts Learned

### Chapter 2: Supervised Learning
1. **Model Architecture:** Linear model with parameters Ï† = [Ï†â‚€, Ï†â‚]
2. **Loss Functions:** Sum-of-squares and Mean Squared Error
3. **Optimization:** Gradient descent with learning rate Î±
4. **Gradient Computation:** âˆ‚L/âˆ‚Ï†â‚€ = (2/m)Â·Î£(errors), âˆ‚L/âˆ‚Ï†â‚ = (2/m)Â·Î£(errorsÂ·x)
5. **Visualization:** Parameter space trajectories on loss landscapes

---

## ğŸ“ˆ Visualization Outputs

### Loss Landscape with Gradient Descent Path
```
- Color intensity: Loss magnitude at each parameter combination
- Gray contours: Constant-loss curves showing optimization region
- Red trajectory: Parameter updates during training iterations
- Green marker: Initial parameter values
- Red star: Converged optimal parameters
```

---

## ğŸ“ Important Notes

- **Learning Rate Selection:** Critical for convergence speed (Î± = 0.02 works well for this problem)
- **Gradient Verification:** Always verify analytical gradients against numerical gradients
- **Convex Optimization:** MSE loss for linear models is convex â†’ guaranteed global optimum
- **Parameter Initialization:** Started at (Ï†â‚€=0.4, Ï†â‚=0.2) or random values
- **Convergence Criterion:** Loss stops decreasing when approaching optimum

---

## ğŸ¯ Next Learning Objectives

- [ ] **Section 2.2:** Multivariate linear regression (multiple input features)
- [ ] **Section 2.3:** Different loss functions (L1, Huber, etc.)
- [ ] **Chapter 3:** Introduction to neural networks and activation functions
- [ ] **Chapter 4:** Backpropagation algorithm and chain rule
- [ ] **Chapter 5:** Deep neural networks and architectures

---

## ğŸ“š Resource Links

- **Official Textbook:** https://udlbook.github.io/udlbook/
- **GitHub Repository:** https://github.com/udlbook/udlbook
- **Chapter 2 (Supervised Learning):** https://udlbook.github.io/udlbook/
- **Code Examples:** Available in official repository

---

## ğŸ“Š Overall Progress

```
Completed: 1/15 chapters
Exercises: 2/40+ notebook cells
Estimated Completion: ~8-10 weeks at current pace
```

**Last Updated:** November 30, 2025  
**Current Chapter:** 2 - Supervised Learning  
**Next Focus:** Section 2.2 - Multivariate Regression

---

## ğŸ” Debugging & Verification Checklist

âœ… Gradient computation verified numerically  
âœ… Loss decreases monotonically during training  
âœ… Final parameters match expected values  
âœ… Visualization shows smooth convergence  
âœ… Design matrix constructed correctly  

---

*This learning journey documents progression through "Understanding Deep Learning" with hands-on implementations and visual explanations.*
